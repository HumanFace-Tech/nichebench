# NicheBench Configuration
# This file controls model settings, evaluation parameters, and global behavior
#
# Repetition Control:
# ------------------
# frequency_penalty (0.0-2.0): Penalizes tokens proportional to how often they appear
#   - 0.0: No penalty (default)
#   - 0.2-0.3: Light penalty for subtle repetition reduction
#   - 0.5-1.0: Moderate penalty for noticeable repetition reduction
#   - 1.0+: Strong penalty, may reduce quality
#
# presence_penalty (0.0-2.0): Penalizes any token that has appeared before
#   - 0.0: No penalty (default)
#   - 0.1-0.3: Light penalty for encouraging diversity
#   - 0.5+: Strong penalty, may reduce coherence
#
# Ollama Integration Setup:
# ------------------------
# 1. Install Ollama: curl -fsSL https://ollama.ai/install.sh | sh
# 2. Start Ollama: ollama serve
# 3. Pull models: ollama pull llama3, ollama pull codellama
# 4. Use profiles: --profile=ollama_local for localhost
# 5. For Docker: Use ollama_docker profile with host.docker.internal
# 6. For remote: Use ollama_remote profile with your server IP
#
# Environment Variables Required:
# ------------------------------
# export GROQ_API_KEY="your_groq_key_here"     # For judge models
# export OPENAI_API_KEY="your_openai_key_here" # For judge models (optional)
#
# Example Commands:
# ----------------
# poetry run nichebench run drupal quiz --profile=ollama_local
# poetry run nichebench run drupal code_generation --profile=ollama_docker

# Model Under Test (MUT) configuration
mut:
  provider: "groq"
  model: "openai/gpt-oss-120b"          # GPT-5 Mini (released August 7, 2025)
  parameters:
    temperature: 0
    max_tokens: 32768
    top_p: 1.0
    frequency_penalty: 0.2              # Reduce repetition by penalizing frequent tokens (0.0-2.0)
    presence_penalty: 0.1               # Encourage diversity by penalizing repeated tokens (0.0-2.0)
    tools: []                           # Empty tools array to force text-only responses
    # Note: reasoning parameters only work with o1 models
    # reasoning_effort: "medium"  # "low", "medium", "high", or null to disable
    # reasoning_format: "hidden"  # "hidden" or "visible" - hidden is ideal for MUT

# Judge model configuration
judge:
  provider: "openai"
  model: "gpt-5"
  parameters:
    temperature: 1.0
    max_tokens: 32768
    top_p: 1.0
    # Judge typically doesn't need reasoning params, but available if needed
    # reasoning_effort: "low"
    # reasoning_format: "visible"

# Global evaluation settings
evaluation:
  save_full_prompts: true       # Include full input prompts in results
  save_raw_outputs: true        # Include raw model outputs
  parallelism: 1                # Number of parallel test executions (1 = sequential)

# Network and reliability settings
network:
  timeout: 600                  # Request timeout in seconds (increased for large token generation)
  retry_attempts: 5             # Number of retry attempts for failed requests (default: 5)
  retry_delay: 3.0             # Base delay between retries in seconds (default: 3.0)
                               # Uses exponential backoff: delay * (2^attempt) with ±20% jitter
                               # Example: 3s → 6s → 12s → 24s → 48s (with random variation)
                               # Retryable errors: timeout, rate limit, server errors, network errors

# Results and reporting
results:
  auto_report: true             # Show rich report after each run
  save_format: "jsonl"          # Format: jsonl, json, csv
  timestamp_format: "%Y%m%d_%H%M%S"

# Framework-specific recommendations (commented examples)
# frameworks:
#   drupal:
#     recommended_mut_models:
#       - {provider: "groq", model: "llama3-70b-8192", temp: 0.0}
#       - {provider: "openai", model: "o1-mini", reasoning_effort: "high"}
#       - {provider: "anthropic", model: "claude-3-5-sonnet-20241022", temp: 0.0}
#
#   wordpress:
#     recommended_mut_models:
#       - {provider: "openai", model: "gpt-4o", temp: 0.1}

# Configuration profiles for different evaluation scenarios
profiles:
  # Fast evaluation with cost-effective models
  groq:
    mut:
      provider: "groq"
      model: "llama-3.1-8b-instant"
      parameters:
        temperature: 0.1
        max_tokens: 2048
    judge:
      provider: "groq"
      model: "meta-llama/llama-4-maverick-17b-128e-instruct"

  # Claude-based evaluation
  anthropic:
    mut:
      provider: "anthropic"
      model: "claude-3-5-sonnet-20241022"
      parameters:
        temperature: 0.0
        max_tokens: 4096
    judge:
      provider: "anthropic"
      model: "claude-3-haiku-20240307"

  # Ollama local setup (default Docker setup)
  ollama_local:
    mut:
      provider: "ollama"
      model: "llama3"                    # Ollama model name (without provider prefix)
      parameters:
        api_base: "http://localhost:11434"  # Default Ollama port
        temperature: 0.0
        max_tokens: 4096
    judge:
      provider: "groq"                   # Use Groq for judge (faster/cheaper)
      model: "llama-3.1-70b-versatile"

  # Ollama Docker container setup (from host machine)
  ollama_docker:
    mut:
      provider: "ollama"
      model: "llama3"
      parameters:
        api_base: "http://host.docker.internal:11434"  # Docker container accessing host
        temperature: 0.0
        max_tokens: 4096
    judge:
      provider: "groq"
      model: "llama-3.1-70b-versatile"

  # Ollama remote server setup
  ollama_remote:
    mut:
      provider: "ollama"
      model: "llama3"
      parameters:
        api_base: "http://192.168.1.100:11434"  # Replace with your Ollama server IP
        temperature: 0.0
        max_tokens: 4096
    judge:
      provider: "groq"
      model: "llama-3.1-70b-versatile"
#
#   budget:
#     mut: {provider: "together", model: "meta-llama/Llama-3-8b-chat-hf", temp: 0.0}
#     judge: {provider: "groq", model: "llama3-70b-8192", temp: 0.5}
