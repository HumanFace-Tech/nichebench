# NicheBench

NicheBench is a lightweight, extensible CLI framework for benchmarking AI models on **framework-specific tasks**. Starting with Drupal, it features LLM-as-a-Judge evaluation, configuration-driven model management, and a rich CLI for interactive reporting.

## âœ¨ Key Features

- **ğŸ¯ LLM-as-a-Judge**: All tasks scored by a second LLM with custom prompts (no regex/heuristics)
- **ï¿½ 3-Value Scoring**: Pass (>66%), Partial (33-66%), Fail (<33%) for nuanced evaluation
- **ğŸ¤– Multi-Turn Conversations**: Agentic code generation with iterative refinement
- **ğŸ›¡ï¸ Runaway Protection**: Automatic detection and handling of repetitive model responses
- **ï¿½ğŸ“¦ Framework Packs**: Plug-and-play support for frameworks (Drupal first, others to follow)
- **âš™ï¸ Configuration-Driven**: YAML-based configuration with profiles for different evaluation scenarios
- **ğŸ”§ Provider Agnostic**: Works with OpenAI, Groq, Anthropic, etc. via `litellm`
- **ğŸ¨ Rich CLI**: Beautiful progress bars, tables, and interactive reporting with stacked results
- **ğŸ” Auto-Discovery**: New frameworks and tasks discovered automatically
- **ğŸ“Š Structured Results**: Detailed JSON/JSONL output for analysis and reproducibility

## ğŸš€ Quick Start

```bash
# Install and run
poetry install
poetry run nichebench list

# View available tasks
poetry run nichebench list drupal

# Run evaluations (uses configuration defaults)
poetry run nichebench run drupal quiz

# Run with specific models
poetry run nichebench run drupal quiz --model groq/llama-3.1-8b-instant --judge openai/gpt-4o

# Use configuration profiles
poetry run nichebench run drupal quiz --profile fast        # Groq models for speed
poetry run nichebench run drupal quiz --profile reasoning   # OpenAI o1 for complex tasks
poetry run nichebench run drupal quiz --profile anthropic   # Claude models

# Run tests
poetry run pytest
```

## âš™ï¸ Configuration

Copy the sample configuration and customize for your needs:

```bash
cp nichebench.sample.yml nichebench.yml
# Edit nichebench.yml with your API keys and preferred models
```

NicheBench uses a `nichebench.yml` configuration file with intelligent defaults and profile system:

```yaml
# Model Under Test (MUT) configuration
mut:
  provider: "groq"
  model: "gemma2-9b-it"
  parameters:
    temperature: 0.0
    max_tokens: 4096

# Judge model configuration
judge:
  provider: "openai"
  model: "gpt-4o"
  parameters:
    temperature: 1.0
    max_tokens: 1024

# Configuration profiles for different scenarios
profiles:
  fast:       # Cost-effective Groq models
    mut: {provider: "groq", model: "llama-3.1-8b-instant"}
    judge: {provider: "groq", model: "llama-3.1-70b-versatile"}

  reasoning:  # OpenAI o1 models with reasoning
    mut:
      provider: "openai"
      model: "o1-preview"
      parameters:
        reasoning_effort: "high"
        reasoning_format: "hidden"
    judge: {provider: "openai", model: "o1-mini"}
```

**Configuration Precedence**: CLI args > Environment variables > Profile > Defaults

## ğŸ§ª Current Status

- **âœ… Drupal Framework Pack**: 2 quiz + 1 code generation task (more coming)
- **âœ… Multi-Turn Code Generation**: Agentic conversations with up to 5 turns
- **âœ… 3-Value Scoring System**: Pass/Partial/Fail evaluation with percentage thresholds
- **âœ… Runaway Protection**: Automatic detection of repetitive/infinite loop responses
- **âœ… Configuration System**: Profile-based model management with YAML configuration
- **âœ… LLM Integration**: Full litellm support with parameter filtering and error handling
- **âœ… Judge-driven Evaluation**: DeepEval-compatible metrics with structured JSON responses
- **âœ… Rich CLI**: Interactive reporting with stacked results and average scores
- **âœ… Test Coverage**: Comprehensive test suite with mocked LLM responses

## ğŸ“ Project Structure

```text
nichebench/
â”œâ”€â”€ nichebench.sample.yml       # Sample configuration file
â”œâ”€â”€ results/                    # Evaluation outputs
â””â”€â”€ src/nichebench/
    â”œâ”€â”€ cli/                    # CLI commands + Rich UI
    â”œâ”€â”€ config/                 # Configuration management
    â”œâ”€â”€ core/                   # Discovery, datamodel, loaders
    â”œâ”€â”€ providers/              # LLM client + judge adapters + conversation management
    â”œâ”€â”€ metrics/                # DeepEval-compatible metrics with 3-value scoring
    â”œâ”€â”€ frameworks/             # Framework packs
    â”‚   â””â”€â”€ drupal/
    â”‚       â”œâ”€â”€ data/           # YAML test cases
    â”‚       â””â”€â”€ prompts/        # System prompts (MUT + Judge)
    â””â”€â”€ utils/                  # Helpers
```

## ğŸ”§ Development

```bash
# Setup development environment
poetry install
poetry run pre-commit install

# Copy sample configuration
cp nichebench.sample.yml nichebench.yml

# Run tests
poetry run pytest

# Code quality checks
poetry run pre-commit run --all-files

# CLI development
poetry run nichebench --help
```

**Requirements**: Python 3.10+, Poetry for dependency management

## ğŸ“ How to Author Tasks

### Test Cases (YAML)

```yaml
# frameworks/drupal/data/quiz/my_quiz.yaml
id: "drupal_quiz_006"
question: "Which API should you use for custom entities in Drupal 11?"
choices:
  - "hook_entity_info()"
  - "EntityTypeInterface annotation"
  - "EntityInterface::create()"
  - "Custom entity plugins"
correct_choice: "B"
context: "You're building a custom module..."
```

### System Prompts (Python)

```python
# frameworks/drupal/prompts/QUIZ.py
QUIZ_SYSTEM_PROMPT = """You are a senior Drupal developer...
Respond with only the letter of your choice (A, B, C, D, or E)."""

# frameworks/drupal/prompts/judges/JUDGE_QUIZ.py
JUDGE_QUIZ_SYSTEM_PROMPT = """You are an expert evaluator...
Respond with JSON: {"pass": true/false, "selected": "B", "score": 1, "explanation": "..."}"""
```

## ğŸ¯ How NicheBench Differs

- **Framework-Specific**: Focus on niche technical domains (Drupal, WordPress, etc.) vs. generic benchmarks
- **3-Value Scoring**: Pass/Partial/Fail evaluation provides nuanced performance insights
- **Multi-Turn Capable**: Supports iterative code generation with conversation management
- **Runaway Protection**: Handles model misbehavior (infinite loops, repetitive responses) gracefully
- **Judge-Centric**: Every evaluation uses LLM-as-a-Judge with custom prompts, not regex matching
- **Configuration-Driven**: Profile system eliminates CLI parameter overload
- **Modular**: Plug-and-play framework packs with auto-discovery
- **Rich UX**: Beautiful CLI with stacked results, average scores, and progress tracking

## ğŸ¤ Contributing

1. Add new framework packs under `src/nichebench/frameworks/<name>/`
2. Create YAML test cases in `data/<category>/` directories
3. Define system prompts in `prompts/` for both MUT and judge
4. Follow the existing Drupal pack structure

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.
