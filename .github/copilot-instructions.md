## 1. Project Overview

* **Name:** NicheBench
* **Goal:** Provide a flexible, extensible CLI framework to benchmark AI models on **framework-specific tasks** (Drupal first, others later).
* **Scope (MVP):** Focus only on **Drupal tasks** (quiz, code generation, bug fixing). Other frameworks may be added later using the same structure.
* **Key Features:**

  * **LLM-as-a-Judge**: Every task is scored by a second LLM with a tailored system prompt. No regex, no string matching.
  * **Multi-turn Conversations**: Code generation and bug fixing use agentic multi-turn conversations with automatic finalization.
  * **3-Value Scoring**: Enhanced pass/partial/fail evaluation (>66%/33-66%/<33%) with criteria-based assessment.
  * **Runaway Protection**: Sophisticated repetition detection prevents infinite loops and excessive token consumption.
  * **Task Types:** quizzes (MCQ), code generation (multi-file, complex), bug fixing (patches).
  * **Dynamic Checklists:** each test case can provide its own evaluation criteria for the judge.
  * **Auto-Discovery:** new frameworks and tasks are discovered automatically from `frameworks/`.
  * **Rich CLI:** all output (progress bars, tables, summaries) uses the `rich` library with stacked result displays.
  * **Provider Agnostic:** powered by `litellm`, works with Groq, Together, Anthropic, OpenAI, etc.

---

## 2. Repository Structure

**src-based Python layout** managed with **Poetry**.

```
nichebench/
├── README.md
├── pyproject.toml          # generated by Poetry
├── nichebench.sample.yml   # sample configuration
├── results/                # run outputs
└── src/
    └── nichebench/
        ├── cli/            # CLI + Rich UI
        ├── core/           # discovery, datamodel, loaders, results
        ├── providers/      # litellm client + judge adapters + conversation management
        ├── metrics/        # custom metrics (quiz, checklist, etc.)
        ├── frameworks/     # framework packs (Drupal, WP, etc.)
        │   └── drupal/
        │       ├── data/   # quiz / codegen / bug YAMLs
        │       ├── prompts/# judge + mut templates
        │       └── registry.py
        ├── config/         # settings, constants
        └── utils/          # helpers
```

* **Framework Packs:** Each framework (Drupal, WordPress, etc.) lives under `frameworks/<name>/` with its own `data/`, `prompts/`, and `registry.py`.
* **Data:** YAML files (`quiz/`, `code_generation/`, `bug_fixing/`) define test cases.
* **Prompts:** Python modules defining system prompts for both MUT (model under test) and Judge.

---

## 3. Workflow Standards

1. **Environment & Tooling:**

   * Always use **Poetry** (`poetry add`, `poetry run`). Never raw pip.
   * Also use poetry to run things (not python -m ...)
   * Python 3.10+.
   * Dependencies: `typer`, `rich`, `deepeval`, `litellm`, `pyyaml`, `pandas`, `pytest`.

2. **Code Style:**

   * Follow PEP8 and project's `pyproject.toml` linting rules.
   * CLI always uses `rich` (tables, progress, panels).
   * Keep modules small and focused.

3. **Development Workflow:**

   * CLI entrypoint: `python -m nichebench`.
   * Add new CLI subcommands in `src/nichebench/cli/commands/`.
   * Test with `pytest -n auto`.

4. **Task Definitions:**

   * YAML schema includes `id`, `prompt`/`question`, `choices` (for quizzes), `correct_choice` or `checklist` for code/bug.
   * Each test case may include a **dynamic checklist** for Judge LLM.
   * System prompts for Judge live in `prompts/`, imported by registry.

5. **Metrics:**

   * **Quiz tasks:** GEval-style metric → judge decides if predicted letter matches gold.
   * **Code/Bug tasks:** Custom checklist metric → judge returns structured JSON with 3-value scoring (pass/partial/fail).
   * **3-Value Scoring:** Criteria can be true (1.0), "partial" (0.5), or false (0.0) for nuanced evaluation.
   * **Overall Score:** Average of all criteria scores, with >66%=pass, 33-66%=partial, <33%=fail categorization.
   * Metrics must subclass deepeval's metric interface.

6. **Results:**

   * Every run writes into `results/<framework>/<task>/<model>/<timestamp>/`.
   * `details.jsonl`: raw per-test results.
   * `summary.json`: aggregated scores.
   * CLI `report` command renders rich tables.

7. **Parallelism:**

   * Use Python `multiprocessing` for parallel jobs.
   * Provide progress updates with `rich.progress`.

8. **Multi-turn Conversations:**

   * Code generation and bug fixing use `ConversationManager` for iterative development.
   * Automatic finalization prompts ensure complete implementations.
   * Runaway protection with chunk-based repetition detection.
   * Maximum turns: 5 for code generation, 4 for bug fixing, 1 for quizzes.

---

## 4. Dataset Management

* **Local Data First:** YAML tasks under `frameworks/<name>/data/`.
* **HF Integration Later:** export to Hugging Face dataset when stable (`nichebench/<framework>_<task>`).
* **Versioning:** `v1.0`, `v1.1` tags per dataset update.

---

## 5. Versioning & Release

* Versions managed in `pyproject.toml`.
* Release workflow:

  1. Bump version
  2. Tag branch (`git tag v0.1.0`)
  3. Publish to PyPI (optional)
  4. Update README changelog

---

## 6. How This Differs From Other Eval Frameworks

* **Not LightEval:** we don't rely on its registry/parallelism.
* **Not HuggingFace Eval Harness:** we don't care about generic benchmarks (MMLU, HellaSwag, etc).
* **Niche Only:** NicheBench is about framework-specific, checklist-driven tasks (Drupal, WP, Svelte, etc).
* **Judge-centric:** Every evaluation uses LLM-as-a-Judge with dynamic criteria, not regex.

---

## 7. Copilot Rules of Thumb

* Always prefer **judge-based evaluation**. Regex/string matching is forbidden.
* Never hardcode provider APIs; always go through `litellm_client`.
* Keep the **CLI** beautiful and clear (progress, results, summaries with `rich`).
* Framework packs must be **plug-and-play** (just drop in `frameworks/foo/`).
* Metrics must be modular and re-usable.
* Don't add unrelated frameworks until requested.
